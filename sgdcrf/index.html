<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="A Java Double Array Trie supporting unicode">

    <link rel="stylesheet" type="text/css" media="screen" href="../css/stylesheet.css">

    <title>Chinese Segmentor</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/fancyerii/chinesesegmentor">View on GitHub</a>

          <h1 id="project_title">Chinese Segmentor</h1>
          <h2 id="project_tagline">a CRFs based Chinese Segmentor using Stochastic Gradient Descent(sgd) algorithm </h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/fancyerii/chinesesegmentor/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/fancyerii/chinesesegmentor/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2><strong>Chinese Segmentor是什么?</strong></h2>

<p><strong>Chinese Segmentor</strong>基于CRFs的中文分词系统，使用sgd训练，速度快，并且支持Online learning和Incremental learning，同时可以使用hadoop实现并行训练</p>

<h2><strong>Chinese Segmentor有哪些特性？</strong></h2>

<ul>
<li><p>训练速度快</p></li>
<li><p>支持大规模数据的训练</p></li>
<li><p>N-best输出</p></li>
<li><p>可视化展示切分（解码）过程</p></li>
</ul>
<h2><strong>安装</strong></h2>
<h3>&nbsp;&nbsp;&nbsp;&nbsp;安装依赖的DoubleArrayTrie</h3>
<p>参考<a href='../doublearraytrie/index.html'>DoubleArrayTrie的文档</a>
</p>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;安装</h3>
<ul>
    <li>下载最新代码</li>
    <li>mvn clean compile assembly:single</li>
</ul>

<h2>用法</h2>
<h3>&nbsp;&nbsp;&nbsp;&nbsp;查看帮助</h3>
java -cp target/chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.SgdCrf
<br/>
<pre>
Usage:
	SgdCrf help
	SgdCrf train <CRF++_format_train_file> <model_file> <crf_train_properties_file> [encoding]
	SgdCrf train2 <tab_sep_text_train_file> <model_file> <crf_train_properties_file> [encoding]
	SgdCrf hdfs-train <hdfs_dir> <model_file> <crf_train_properties_file> <feature_dict> [encoding] [hdfsconf1] [hdfsconf2] ...
	SgdCrf test  <test_file> <model_file> [encoding]
	SgdCrf tag <model_file> [nBest] [encoding]
</pre>
<h3>&nbsp;&nbsp;&nbsp;&nbsp;训练数据格式</h3>
<p>
目前支持两种格式的训练数据：<a href='http://crfpp.googlecode.com/svn/trunk/doc/index.html'>CRF++</a>格式的数据；tab分割的数据。
</p>
<p>
  人民日报的数据已经被处理成合适的格式了，使用了6标签：B E S B1 B2 M，即单字词为S，两字词为B E，三字词为B B1 E，四字词为 B B1 B2 E，五字词为B B1 B2 M E，...<br/><br/>
  句子之间用一个空行分割开来。<br/><br/>
  关于tag的比较请参考 Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling by Hai Zhao et al.(2006)
</p>

<p>人民日报语料的片段
<pre>
天	B
气	E
趋	B
势	E
分	B
析	E

据	S
新	B
华	B1
社	E
天	B
津	E
１	B
</pre>
人民日报的句子较长，如果为了提高效果可以先用逗号等一些标点符号先预处理，然后在训练效果可能更好。我这里只是用来测试而已（人民日报的语料库和互联网的差别很大，所以如果想要用于互联网的话需要增加相关的训练数据）
</p>
 
<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>人民日报分词语料库的训练</em></h3>
<p>训练的参数都在配置文件conf/crf_train.properties里，具体参数请参考下面</p>
<pre>java -cp target/chinesesegmentor-1.0-jar-with-dependencies.jar -Xmx2g com.antbrains.crf.SgdCrf train data/people-daily.train model/people-daily.model conf/crf_train.properties</pre>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>测试</em></h3>
<pre>java -cp target/chinesesegmentor-1.0-jar-with-dependencies.jar -Xmx2g com.antbrains.crf.SgdCrf test data/people-daily.test model/people-daily.model</pre> 

<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>Tab分隔的训练数据</em></h3>
<p>训练的参数都在配置文件conf/crf_train.properties里，具体参数请参考下面</p>
<pre>java -cp target/chinesesegmentor-1.0-jar-with-dependencies.jar -Xmx2g com.antbrains.crf.SgdCrf train2 tab-sep-trainingdata model/people-daily.model conf/crf_train.properties</pre>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>分词</em></h3>
<pre>java -cp target/chinesesegmentor-1.0-jar-with-dependencies.jar -Xmx2g com.antbrains.crf.SgdCrf seg model/people-daily.model</pre> 

<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>查看切分路径</em></h3>
<pre>java -cp target/chinesesegmentor-1.0-jar-with-dependencies.jar -Xmx2g com.antbrains.crf.CRFExplainer model/people-daily.model</pre>
<img src='../images/crf.png'/>
<p>说明：
<ul>
<li>点击分词后然后分析</li>
<li>点击某个节点，比如图上的”今/B“</li>
我们可以得到今字的tag是B的概率。其中U04:今/天的值是2.4，U04表示当前字和下一个字的组合（具体参考Template），也就是说如果当前字是”今“，下一个字是”天“，那么今被标注为B的概率是比较大的。
<pre>
total score: 3.6785386417228456
U04:今/天  2.4
U01:今  1.3
U02:天  -.1
</pre>
<li>点击某条边，我们可以知道tag直接的转移概率</li>
<li>我们也可以修改某些边(右键删除边，托放增加边）来分析其它路径的概率</li>
</ul>
</p>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>训练参数说明</em></h3>
<pre>
# feature
mininumFeatureFrequency=1 //如果一个特征的出现次数小于它会被过滤，默认1不会过滤任何特征，过滤有助于减少特征大小，加快学习速度和分词速度，也有可能抑制过拟合。

# sgd
sigma=5.0 
eta=0.1
rate=2.0
iterateCount=100 //迭代次数

# calibrate //确定t0的过程
samplesNum=10000  
candidatesNum=10


templateFile=./conf/segment.tmpl //模版
template.charset: utf8
</pre>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;<em>模版说明</em></h3>
<p>和CRF++格式兼容，但不支持B，只支持U</p>
<pre>
U00:%x[-1,0]
U01:%x[0,0]
U02:%x[1,0]
U03:%x[-1,0]/%x[0,0]
U04:%x[0,0]/%x[1,0]
U05:%x[-1,0]/%x[1,0]
</pre>

<h2>基于Hadoop的并行训练</h2>
<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 0：搭建环境</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;首先需要准备好hadoop的运行环境，建议至少搭建一个5个结点以上的机群，至少有1T的磁盘总量，代码使用的是hadoop 1.2.1，所以如果是其它版本的api（尤其是早期的版本，比如0.20.x），可能api不兼容</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;另外需要一台机器运行一些非mapreduce任务，需要有5G以上内存（如果训练的数据非常多，需要内存量更大）</p>
<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 1：准备数据 train.txt</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;为了演示，我准备了一个10,000,000行的训练数据，由于版权问题，不能公布训练数据，请使用自己训练数据，数据格式为：用TAB分割的句子每行一个</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;小提示：<em>如果你没有大量的训练数据，但是有一个较好的分词软件，那么你可以用这个分词软件生成训练数据（当然如果有人力修正错误就更好了）</em></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;把它拷贝到hdfs上，以便后续使用，我这里假设数据放到/lili/train.txt</p>
<pre>hadoop fs -put train.txt /lili/</pre>
<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 2：FeatureCounter</h3>
<p>首先需要生成feature，然后统计每个feature的数量，为下一步的过滤做准备
</p>
<pre>./bin/hadoop fs -rmr /lili/feature_counter
./bin/hadoop jar chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.FeatureCounter -Dmapred.reduce.tasks=32 -Dmapred.child.java.opts=-Xmx1g -Dmapred.output.compress=true -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec /lili/train.txt /lili/feature_counter seg.template</pre> 
seg.template的内容如下<br/>
<pre>
U00:%x[-1,0]
U01:%x[0,0]
U02:%x[1,0]
U03:%x[-1,0]/%x[0,0]
U04:%x[0,0]/%x[1,0]
U05:%x[-1,0]/%x[1,0]
</pre>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 3：FeatureFilter(可选)</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;如果生成的feature过多，那么可以使用这一步过滤掉一些出现频率较低的feature，当然也可以跳过这一步。我的示例中跳过了此步。
<br/>&nbsp;&nbsp;&nbsp;&nbsp;说明：如果feature数量超过Integer.MAX_VALUE/LABEL_NUM(default 6)，那么必须过滤。因为目前我是使用double[]来存储权重的，而Java的数组的长度是2G。以后考虑支持更大的特征数
</p>
<pre>
./bin/hadoop fs -rmr /lili/feature_filter
./bin/hadoop jar chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.FeatureFilter -Dmapred.child.java.opts=-Xmx2g -Dmapred.output.compress=true -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec /lili/feature_counter /lili/feature_filter filter_rules.txt false
</pre>
过滤规则文件：filter_rules.txt内容如下<br/>
<pre>
U03 100
U04 100
U05 100
</pre>
说明：U03 100说明对于U03展开的feature，出现次数如果小于100，那么就过滤掉

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 4：FeatureDictReader</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;这一步生成feature dict
</p>

<pre>
java -Xmx4g -cp ".:./*" com.antbrains.crf.hadoop.FeatureDictReader /lili/feature_counter ./featuredict $HADOOP_HOME/conf/core-site.xml $HADOOP_HOME/conf/hdfs-site.xml $HADOOP_HOME/conf/mapred-site.xml
</pre>
说明：需要把$HADOOP_HOME替换成HADOOP的目录
说明：需要记住其中输出的feature数量，下一步需要使用
比如我的输出是：9109259=?=9109259，那么就需要记住9109259这个数字。

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 5：把生成的Feature Dict放到hdfs中</h3>
<pre>hadoop fs -put ./featuredict /lili/</pre>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 6：InstanceGenerator</h3>
<pre>
./bin/hadoop fs -rmr /lili/instance_gen
./bin/hadoop jar chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.InstanceGenerator -Dmapred.child.java.opts=-Xmx4g -Dmapred.tasktracker.map.tasks.maximum=1 -Dmapred.task.timeout=1200000 -Dmapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec -Dmapred.map.tasks=8  -Dmapred.job.reuse.jvm.num.tasks=-1 -Dmapred.output.compress=true -Dmapred.map.tasks.speculative.execution=false /lili/train.txt /lili/instance_gen /lili/featuredict seg.template
</pre>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 7：ParallelTraining2 并行训练（注意类名2，最早写的ParallelTraining有问题）</h3>
<pre>
./bin/hadoop fs -rmr /lili/ptrain
./bin/hadoop jar chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.ParallelTraining2 -Dmapred.child.java.opts=-Xmx4g -Dmapred.map.tasks=8   -Dmapred.task.timeout=1800000 -Dmapred.job.reuse.jvm.num.tasks=-1 -Dmapred.map.tasks.speculative.execution=false /lili/instance_gen /lili/ptrain_bd 9109259 crf_train.properties 1
</pre>

crf_train.properties的内容：
<pre>
# feature
mininumFeatureFrequency=1

# sgd
sigma=5.0
eta=0.1
rate=2.0
iterateCount=100

# calibrate
samplesNum=100000
candidatesNum=10

templateFile=./seg.template
template.charset: utf8
</pre>

说明：可以调整的参数包括iterateCount迭代次数。其它的参数请阅读论文，了解用法后修改。<br/>
说明：9109259是前面那步的输出， 最后的参数1说明只迭代一次（我试过，多次迭代作用不大）<br/>
说明：如果发现mapper在100%之后没有动静请不要着急，因为所有的工作都是在mapper完了才进行的，如果你想看训练进度，可以看hadoop的log

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 8：根据Feature的权重裁剪（可选）</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp;经过训练，我们得到的模型写到磁盘有500多MB，加载到内存需要4G的内存，我们的机器可能不能提高这么多内存，所以提供了一个步骤来剪裁特征<br/>
剪裁的思路就是：如果某个feature,比如U00:中的所有权重的绝对值都很小（比如都是0），那么这个特征其实并不重要<br/>
</p>
<h4>&nbsp;&nbsp;&nbsp;&nbsp;Step 8.1 ReverseFeatureDict</h4>
<pre>
java -Xmx4g -Xms2g -cp chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.ReverseFeatureDict ./featuredict ./reversed_featuredict
</pre>

<h4>&nbsp;&nbsp;&nbsp;&nbsp;Step 8.2 CalcFeatureWeights 计算特征权重</h4>
<pre>
./bin/hadoop jar chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.CalcFeatureWeights /lili/ptrain/result1 /lili/feature_weights_tmp /lili/feature_weights
</pre>

<h4>&nbsp;&nbsp;&nbsp;&nbsp;Step 8.3 TruncateModel 裁剪模型</h4>
<pre>
java -Xmx5g -cp chinesesegmentor-1.0-jar-with-dependencies.jar com.antbrains.crf.hadoop.TruncateModel ./reversed_featuredict /lili/feature_weights 2000000 ./small.model ./crf_train.properties /lili/ptrain/result1 $HADOOP_HOME/conf/core-site.xml $HADOOP_HOME/conf/hdfs-site.xml $HADOOP_HOME/conf/mapred-site.xml
</pre>
说明： 2000000说明只保留前2百万的feature（之前将近一千万,9109259）

<h3>&nbsp;&nbsp;&nbsp;&nbsp;Step 9：测试模型</h3>
<pre>java -cp chinesesegmentor-1.0-jar-with-dependencies.jar -Xmx4g com.antbrains.crf.SgdCrf test2 train.txt small.model
</pre>

未裁剪的模型在训练数据上的测试结果：
<pre>
S: precision=94.0369% (16822970/17889753); recall=96.6108% (16822970/17413128)
B: precision=98.5330% (33164248/33658010); recall=96.8260% (33164248/34251376)
E: precision=98.3295% (33095759/33658010); recall=96.6261% (33095759/34251376)
B1: precision=93.8809% (9868285/10511490); recall=95.7300% (9868285/10308451)
B2: precision=90.4860% (4615002/5100238); recall=96.3451% (4615002/4790072)
M: precision=93.4992% (4244034/4539112); recall=97.7390% (4244034/4342210)
item accuracy = 96.6340% (101810298/105356613); sentence accuracy = 86.4176% (8641763/10000000)
</pre>
裁剪后的模型的结果：
<pre>
S: precision=93.7776% (16776573/17889753); recall=96.5409% (16776573/17377691)
B: precision=98.4980% (33152472/33658010); recall=96.7558% (33152472/34264070)
E: precision=98.2880% (33081798/33658010); recall=96.5495% (33081798/34264070)
B1: precision=93.8375% (9863718/10511490); recall=95.6404% (9863718/10313335)
B2: precision=90.4625% (4613805/5100238); recall=96.2784% (4613805/4792152)
M: precision=93.4956% (4243872/4539112); recall=97.6659% (4243872/4345295)
item accuracy = 96.5599% (101732238/105356613); sentence accuracy = 86.1224% (8612241/10000000)
</pre>

<em>可以发现，我们裁剪后准确率和召回率稍有下降，但是模型小了很多。裁剪后的模型只有100多M，加载到内存不到1G</em>

<h3>&nbsp;&nbsp;&nbsp;&nbsp;模型下载</h3>
如果你没有训练数据，可以下载我训练好的模型（如前面所说，由于版权问题不能提供训练数据），<a href='http://yun.baidu.com/share/link?shareid=1408372168&uk=3204611783'>模型下载地址</a>

<h2>Future Road Map</h2>
<ul>
    <li>支持Lucene的Analyzer</li>
    <li>预处理（语言识别，分句，分词，数字/日期等）</li>
    <li>增量学习（包括增加新词典）</li>
    <li>通过网络发现新词</li>

</ul>

<h2>参考资料</h2>
<ul>
<li><a href='http://www.chokkan.org/software/crfsuite/'>CRF Suite</a> 本程序参考了CRFSuite的代码实现</li>
<li><a href='http://crfpp.googlecode.com/svn/trunk/doc/index.html'>CRF++</a></li>
<li><a href='http://leon.bottou.org/projects/sgd'>Léon Bottou的sgd项目</a></li>

<li>J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, In Proc. of ICML, pp.282-289, 2001</li>
<li>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. “Pegasos: Primal Estimated sub-GrAdient SOlver for SVM”. Proceedings of the 24th International Conference on Machine Learning (ICML 2007). 807-814. 2007.</li>


</ul>
</section>
</div>
</body></html>
